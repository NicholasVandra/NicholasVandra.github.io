<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="audio-sentiment-analysis">Audio Sentiment Analysis</h1> <p>This project focuses on detecting <strong>sentiment directly from raw speech audio</strong>, not text transcripts.<br> The goal is to capture emotional cues such as tone, pitch, energy, and cadence — aspects that traditional text-only models cannot detect.</p> <hr> <h2 id="motivation">Motivation</h2> <p>Written text removes essential emotional information present in the human voice.<br> To address this gap, I built a model that performs <strong>emotion and sentiment classification from audio alone</strong>, enabling:</p> <ul> <li>More expressive conversational AI</li> <li>Enhanced customer-service classification</li> <li>Emotion-aware agents and assistants</li> <li>Better human-AI interaction</li> </ul> <p>This research later became part of a <strong>GPT Agent</strong> that performs real-time audio sentiment analysis.</p> <hr> <h2 id="model-architecture">Model Architecture</h2> <h3 id="feature-inputs"><strong>Feature Inputs</strong></h3> <ul> <li> <strong>MFCCs</strong> – capture vocal tract characteristics and emotional tone</li> <li> <strong>Mel Spectrograms</strong> – capture frequency-energy distribution</li> </ul> <p>These are fused to form a rich representation of the input audio.</p> <h3 id="neural-network-pipeline"><strong>Neural Network Pipeline</strong></h3> <ul> <li> <strong>CNN layers</strong> to detect local temporal–frequency features</li> <li> <strong>BiLSTM layers</strong> for long-range temporal understanding</li> <li><strong>Fully-connected classification head</strong></li> <li>Output: <strong>3–7 sentiment/emotion classes</strong> (depending on dataset)</li> </ul> <hr> <h2 id="datasets">Datasets</h2> <ul> <li> <strong>MELD</strong> (Multimodal EmotionLines Dataset)</li> <li>Custom curated speech samples</li> </ul> <p>Both are preprocessed into MFCC + Mel fusion inputs.</p> <hr> <h2 id="project-status">Project Status</h2> <ul> <li>Neural network is fully trained and operational</li> <li>Integrated into an <strong>OpenAI GPT Agent</strong> for real-time inference</li> <li>Evaluated on multiple emotional classes</li> <li>Currently exploring cross-dataset generalization</li> </ul> <hr> <h2 id="future-work">Future Work</h2> <ul> <li>Add self-supervised pretraining for better representation learning</li> <li>Combine with transformer-based audio encoders</li> <li>Expand emotion categories</li> <li>Deploy as a cloud microservice</li> </ul> </body></html>