<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="overview">Overview</h2> <p>This project introduces a <strong>new type of audio transformer architecture</strong> designed to process raw audio and perform multiple downstream tasks through a multi-headed decoding framework. The model combines CNNs, BiLSTMs, and Transformer layers to capture both local acoustic features and long-range temporal dependencies.</p> <p>This is my primary ongoing research project as a <strong>Data Science graduate student</strong> and aspiring <strong>Machine Learning Engineer</strong>.</p> <hr> <h2 id="architecture-summary">Architecture Summary</h2> <h3 id="1-feature-extraction"><strong>1. Feature Extraction</strong></h3> <p>A hybrid <strong>CNN + BiLSTM</strong> extractor captures:</p> <ul> <li>Local spectral patterns (via CNN)</li> <li>Temporal dependencies (via BiLSTM)</li> <li>Combined mel-spectrogram + MFCC features</li> </ul> <h3 id="2-positional-encoding"><strong>2. Positional Encoding</strong></h3> <p>Features are converted to sequences and embedded using sinusoidal positional encoding.</p> <h3 id="3-transformer-encoder"><strong>3. Transformer Encoder</strong></h3> <ul> <li>Multi-head self-attention</li> <li>Feed-forward layers</li> <li>Layer normalization + dropout</li> <li>Trained on large mixed datasets (Common Voice, LibriTTS, MELD)</li> </ul> <h3 id="4-multi-decoder-system-novel-component"><strong>4. Multi-Decoder System (Novel Component)</strong></h3> <p>Each decoder specializes in a different downstream task:</p> <ul> <li> <strong>Speech Recognition Decoder</strong> → predicts phoneme sequences</li> <li> <strong>Sentiment Analysis Decoder</strong> → predicts emotional tone</li> <li> <strong>Text Generation Decoder</strong> → generates language conditioned on audio</li> </ul> <p>Finally, a <strong>fusion decoder</strong> merges outputs to produce a robust final inference.</p> <hr> <h2 id="goals--contributions">Goals &amp; Contributions</h2> <ul> <li>Build a fully raw-audio transformer without relying on text preprocessing.</li> <li>Explore multi-task learning in audio systems.</li> <li>Improve representation learning through hybrid encoders.</li> <li>Train using memory-safe streaming pipelines for massive datasets.</li> <li>Investigate decoder fusion as a new architecture style in audio ML.</li> </ul> <hr> <h2 id="technologies-used">Technologies Used</h2> <ul> <li><strong>Python, TensorFlow, PyTorch</strong></li> <li><strong>Mel Spectrograms, MFCC</strong></li> <li><strong>Transformers / Attention Mechanisms</strong></li> <li><strong>GPU Training (NVIDIA A6000)</strong></li> <li><strong>Distributed data loading (WSL, memmap)</strong></li> <li><strong>Common Voice + LibriTTS + MELD datasets</strong></li> </ul> <hr> <h2 id="current-status">Current Status</h2> <ul> <li> <strong>Feature extraction network fully completed</strong> (MFCC + Mel fusion with CNN–BiLSTM pipeline)</li> <li><strong>Transformer encoder fully trained, validated, and stress-tested</strong></li> <li><strong>Speech recognition decoder built and currently undergoing evaluation</strong></li> <li><strong>Multi-headed decoder framework implemented</strong></li> <li> <strong>Actively developing the additional decoder heads</strong> for <ul> <li>sentiment analysis</li> <li>text generation</li> <li>classification</li> <li>other downstream audio-based tasks</li> </ul> </li> <li>Preparing full evaluation metrics, cross-dataset tests, and ablation studies</li> </ul> <hr> <h2 id="future-plans">Future Plans</h2> <ul> <li>Implement beam search with improved phoneme handling</li> <li>Add multilingual audio support</li> <li>Build demo web interface for real-time speech recognition</li> <li>Publish a paper on the multi-headed transformer architecture</li> </ul> <hr> <p>Just tell me which ones to include!</p> </body></html>